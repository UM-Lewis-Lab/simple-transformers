{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a401d4d5-48b7-47c1-955f-da124ca2e980",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%javascript\n",
    "require.config({\n",
    "  paths: {\n",
    "      d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.8/d3.min',\n",
    "      jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
    "  }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ef98a8-8114-403d-8dba-c868ed221abb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import bertviz\n",
    "\n",
    "DATA_PATH = Path(\"../test-output/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c4dcae-4f3c-4e03-8a2f-abcc4c05b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pd.read_csv(DATA_PATH, index_col=\"id\")\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ea7b11-c4a1-489a-891c-f612127a73c7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def view_attention(prompt_id: int, head_view: bool):\n",
    "    # Load the attentions\n",
    "    data = torch.load(DATA_PATH.parent / (DATA_PATH.name.split(\".\")[0] + f\"_attn{prompt_id}.pt\"))\n",
    "    tokens = data[\"prompt_tokens\"] + data[\"response_tokens\"]\n",
    "    n_prompt = len(data[\"prompt_tokens\"])\n",
    "    n_response = len(data[\"response_tokens\"])\n",
    "    n_tokens = n_prompt + n_response\n",
    "    n_layers = len(data[\"attention\"][0])\n",
    "    n_heads = data[\"attention\"][0][0].size(1)\n",
    "\n",
    "\n",
    "    # Attentions for each generated token are separate\n",
    "    # 1xN tensors. Combine them into one large NxN\n",
    "    # tensor, padding the empty space with zeros. \n",
    "    layers = []\n",
    "    for i in range(n_layers):\n",
    "        layer_attn = []\n",
    "        for token_attn in data[\"attention\"]:\n",
    "            token_layer_attn = token_attn[i]\n",
    "            layer_attn.append(\n",
    "                torch.nn.functional.pad(\n",
    "                    token_layer_attn,\n",
    "                    (0, n_tokens - token_layer_attn.size(-1)),\n",
    "                    mode=\"constant\",\n",
    "                    value=0,\n",
    "                )\n",
    "            )\n",
    "        layer_attn.insert(0, torch.zeros((1,n_heads,1,n_tokens)))\n",
    "        layers.append(torch.cat(layer_attn, dim=-2).transpose(-1, -2))\n",
    "    if head_view:\n",
    "        return bertviz.head_view(layers, tokens)\n",
    "    else:\n",
    "        return bertviz.model_view(layers, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af73318a-b96b-4450-b93f-6366684883d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model View\n",
    "<b>The model view provides a birds-eye view of attention throughout the entire model</b>. Each cell shows the attention weights for a particular head, indexed by layer (row) and head (column).  The lines in each cell represent the attention from one token (left) to another (right), with line weight proportional to the attention value (ranges from 0 to 1).  For a more detailed explanation, please refer to the [blog](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1).\n",
    "\n",
    "## Usage\n",
    "ðŸ‘‰ **Click** on any **cell** for a detailed view of attention for the associated attention head (or to unselect that cell). <br/>\n",
    "ðŸ‘‰ Then **hover** over any **token** on the left side of detail view to filter the attention from that token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e694b0-fb99-42e3-a9e9-4ff568de613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_attention(prompt_id=0, head_view=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c194aa28-95b4-471b-9c6d-eff1c990ea28",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Head View\n",
    "<b>The head view visualizes attention in one or more heads from a single Transformer layer.</b> Each line shows the attention from one token (left) to another (right). Line weight reflects the attention value (ranges from 0 to 1), while line color identifies the attention head. When multiple heads are selected (indicated by the colored tiles at the top), the corresponding  visualizations are overlaid onto one another.  For a more detailed explanation of attention in Transformer models, please refer to the [blog](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1).\n",
    "\n",
    "## Usage\n",
    "ðŸ‘‰ **Hover** over any **token** on the left/right side of the visualization to filter attention from/to that token. <br/>\n",
    "ðŸ‘‰ **Double-click** on any of the **colored tiles** at the top to filter to the corresponding attention head.<br/>\n",
    "ðŸ‘‰ **Single-click** on any of the **colored tiles** to toggle selection of the corresponding attention head. <br/>\n",
    "ðŸ‘‰ **Click** on the **Layer** drop-down to change the model layer (zero-indexed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0361bbc8-2f57-4224-919a-86e92477474e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "view_attention(prompt_id=0, head_view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355e9a6d-b62d-475b-9caf-2534deaa5fb2",
   "metadata": {},
   "source": [
    "# Credit\n",
    "Attention analysis interfaces and descriptions taken from the [BERTViz Interactive Tutorial](https://colab.research.google.com/drive/1hXIQ77A4TYS4y3UthWF-Ci7V7vVUoxmQ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
